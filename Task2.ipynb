{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Thư viện\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'attraction-find_attraction': 0, 'bus-find_bus': 1, 'police-find_police': 2, 'train-book_train': 3, 'hotel-book_hotel': 4, 'hotel-find_hotel': 5, 'hospital-find_hospital': 6, 'train-find_train': 7, 'taxi-find_taxi': 8, 'restaurant-book_restaurant': 9, 'restaurant-find_restaurant': 10}\n",
      "All labels: ['attraction-find_attraction', 'bus-find_bus', 'police-find_police', 'train-book_train', 'hotel-book_hotel', 'hotel-find_hotel', 'hospital-find_hospital', 'train-find_train', 'taxi-find_taxi', 'restaurant-book_restaurant', 'restaurant-find_restaurant']\n",
      "Vocab size: 7424\n"
     ]
    }
   ],
   "source": [
    "# Chuẩn bị dữ liệu\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def create_label_map(data):\n",
    "    unique_labels = set()\n",
    "    for _, labels in data:\n",
    "        for label in labels:\n",
    "            unique_labels.add(label)\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    return label_map, list(unique_labels)\n",
    "\n",
    "\n",
    "def encode_labels(labels, label_map):\n",
    "    encoded_labels = np.zeros(len(label_map), dtype=int)\n",
    "    for label in labels:\n",
    "        encoded_labels[label_map[label]] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "def build_vocab(data):\n",
    "    words = []\n",
    "    for sentence, _ in data:\n",
    "        words.extend(sentence.lower().split())\n",
    "    word_counts = Counter(words)\n",
    "    vocab = {word: i + 2 for i, word in enumerate(word_counts)}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def encode_sentence(sentence, vocab):\n",
    "    tokens = sentence.lower().split()\n",
    "    encoded = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    return encoded\n",
    "\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, data, vocab, label_map, max_length):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, labels = self.data[idx]\n",
    "        encoded_sentence = encode_sentence(sentence, self.vocab)\n",
    "        padded_sentence = self.pad_sequence(encoded_sentence, self.max_length)\n",
    "        encoded_labels = encode_labels(labels, self.label_map)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(padded_sentence, dtype=torch.long),\n",
    "            'labels': torch.tensor(encoded_labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def pad_sequence(self, seq, max_length):\n",
    "        padded = seq[:max_length]\n",
    "        padding = [self.vocab['<pad>']] * max(0, max_length - len(padded))\n",
    "        return padded + padding\n",
    "\n",
    "filepath = 'E://NLP//AS3//Code//all_samples.json'  \n",
    "data = load_data(filepath)\n",
    "\n",
    "label_map, all_labels = create_label_map(data)\n",
    "print(f\"Label map: {label_map}\")\n",
    "print(f\"All labels: {all_labels}\")\n",
    "\n",
    "vocab = build_vocab(data)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#chia tập train,test,val\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "max_length = 38 # câu dài nhất là 38 \n",
    "train_dataset = IntentDataset(train_data, vocab, label_map, max_length)\n",
    "val_dataset = IntentDataset(val_data, vocab, label_map, max_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(7424, 100)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mô hình LSTM\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size) # nhân 2 lần lên vì dùng bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2 # phân loại câu nên 1 đến 2 layer thôi \n",
    "output_size = len(label_map)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_size, num_layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 1/20, Training Loss: 0.02808610163629055, Validation Loss: 0.12409220146441301, F1: 0.7713055656413714\n",
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 2/20, Training Loss: 0.22237053513526917, Validation Loss: 0.11689345940375367, F1: 0.7944967266775778\n",
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 3/20, Training Loss: 0.11930058896541595, Validation Loss: 0.11410469462269472, F1: 0.7941102059403936\n",
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-book_hotel']\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 4/20, Training Loss: 0.03252384439110756, Validation Loss: 0.11383611393683529, F1: 0.7944314602174576\n",
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 5/20, Training Loss: 0.2728387415409088, Validation Loss: 0.1163342025314453, F1: 0.7995431749342072\n",
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 6/20, Training Loss: 0.07545999437570572, Validation Loss: 0.12106557790724831, F1: 0.7958818263205013\n",
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Epoch 7/20, Training Loss: 0.09116062521934509, Validation Loss: 0.1289001529475049, F1: 0.7940585322809942\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vieth\\AppData\\Local\\Temp\\ipykernel_9240\\3553865515.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Predicted Labels: []\n",
      "Actual Labels: ['hotel-find_hotel', 'restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-book_hotel']\n",
      "Actual Labels: ['restaurant-find_restaurant']\n",
      "Predicted Labels: ['hotel-find_hotel']\n",
      "Actual Labels: ['hotel-find_hotel']\n",
      "Predicted Labels: ['train-find_train']\n",
      "Actual Labels: ['train-find_train']\n",
      "Final F1 score: 0.7944314602174576\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device, all_labels):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids)\n",
    "        # Sử dụng threshold để quyết định nhãn dự đoán. Do một câu có thể có nhiều nhãn nên với mỗi nhãn, xác suất > 0.5 được coi là nhãn đó xuất hiện.\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).cpu().numpy() \n",
    "        targets = labels.cpu().numpy()\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, labels).item()\n",
    "        total_loss += loss\n",
    "        all_preds.extend(predictions)\n",
    "        all_targets.extend(targets)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    f1 = f1_score(all_targets, all_preds, average='micro')\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    def get_predicted_labels(predictions, all_labels):\n",
    "      predicted_labels = []\n",
    "      for pred in predictions:\n",
    "        predicted_labels_for_example = []\n",
    "        for i, label_prob in enumerate(pred):\n",
    "          if label_prob == 1:\n",
    "            predicted_labels_for_example.append(all_labels[i])\n",
    "        predicted_labels.append(predicted_labels_for_example)\n",
    "      return predicted_labels\n",
    "    \n",
    "    predicted_labels = get_predicted_labels(all_preds, all_labels)\n",
    "    \n",
    "    print(\"Sample predictions:\")\n",
    "    for i in range(min(5, len(all_preds))):\n",
    "        print(f\"Predicted Labels: {predicted_labels[i]}\")\n",
    "        print(f\"Actual Labels: {get_predicted_labels([all_targets[i]], all_labels)[0]}\")\n",
    "\n",
    "    return f1, avg_loss\n",
    "    \n",
    "\n",
    "# Cài early stop dừng lại nếu sau 3 epoch mà kết quả tiến triển không tốt\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "epochs = 20  # cài thật lớn epoch và để cho chạy đến khi không cải thiện nữa thì tự dừng nhờ early stop \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # F1-score\n",
    "    f1, val_loss = evaluate_model(model, val_dataloader, device, all_labels)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss}, F1: {f1}\")\n",
    "\n",
    "    # Giữ model tốt nhất \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "\n",
    "    if trigger_times >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "# Đánh giá \n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "f1_score_final, loss_final = evaluate_model(model, val_dataloader, device, all_labels)\n",
    "print(f\"Final F1 score: {f1_score_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
